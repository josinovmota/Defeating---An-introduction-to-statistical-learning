{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilidade geral da classificação\n",
    "\n",
    "$Pr(Y=k|X=x)$\n",
    "\n",
    "## Termos da classificação em geral:\n",
    "\n",
    "$Y$ = **Target Value** Valor que estamos buscando prever a probabilidade para cada **classe**\n",
    "\n",
    "$k$ = **Classes** em que vamos classificar os nossos dados\n",
    "\n",
    "$X$ = **Set of Predictor Variables**, ou seja, representa o conjuntos de predictors para cada N(dados)\n",
    "\n",
    "$x$ = **Predictors** no caso de haver 2 ou mais predictors, esse x pode ser representado como $x_1$, $x_2$ ... $x_p$.\n",
    "\n",
    "$πK$ = **Prior Probability** probabilidade de encontrar dados de cada classe dentro do conjunto de dados\n",
    "\n",
    "$f_k(x)$ = **Class Conditional Probability** assimilação dos predictors de cada dado às classes K escolhidas\n",
    "\n",
    "$pk(x)$ - Abreviação de Pr(Y = k|X = x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por que não usar regressão linear\n",
    "\n",
    "Um dos maiores fatores que são considerados na escolha de uma regressão linear como modelo é o fato dos predictors serem **Quantitativos**. Na regressão existem formas de lidar com predictors quantitativos, por meio de dummy variables, porém, muitas vezes as dummy variables não conseguem mostrar toda a diferença entre os predictors. Um exemplo disso é expressar a diferença de uma doença para outra por meio de uma dummy variable. No caso da dummy variable, serão atribuidos valores -1 e 1 para as doenças, valores esses que não conseguem expressar tanto a diferença como as probabilidades manifestam dentro da classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood\n",
    "\n",
    "Assim como na **Regressão** utilizamos o método **Least Squares**, na **Classificação** usamos o **Maximum Likelihood** que busca as maiores probabilidades dos predictors para uma tal classe K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Uma das maiores características da **Logistic Regression Simples** é que ela é usada somente quanto temos $K <= 2$, ou seja, caso busquemos classificar mais de 2 classes, não poderemos usar a abordagem da **Logistic Regression**. Isso tudo faz com que ela seja mais comumente usada quando temos classes **Binárias**, como \"Yes | No\"\n",
    "\n",
    "Seu cálculo é simples e é expresso por:\n",
    "\n",
    "$log(\\frac{p(X)}{1-p(X)}) = B_0 + B_1X_1$\n",
    "\n",
    "Caso tenhamos $K > 2$ partimos para a **Multinomial Logistic Regression** e por sua parte matemática ser mais complexa como as que vão aparecer adiante, não as cito aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models For Classification\n",
    "\n",
    "Nessa modalidade, buscamos prever a probabilidade de cada predictor se assimilar com cada classe K afim de classificar cada Dado em sua classe com maior probabilidade\n",
    "\n",
    "Quando o valor as classes K são bem distintas uma das outras, essa abordagem tende a ser melhor que a logistic regression\n",
    "\n",
    "### Linear Discriminant Analysis\n",
    "\n",
    "Assume o nome Linear pois sua função discriminante é linear, diferentemente da QDA que será apresentada em breve\n",
    "\n",
    "Ela faz aproximações do πk(Prior Probability), µk(Mean Centroid) e σ2(Standard Deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
